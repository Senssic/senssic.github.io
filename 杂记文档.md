---
title: 杂记文档
date: 2018-10-15 14:26:46
tags: [杂记,记录]
categories: [杂记]
password: senssic
message: 「密码不能告诉你」
---



# 缓存队列相关

## 1.为什么使用mq？mq的优点？

解耦,异步,削峰(可以限流发送或接收)

## 2.mq的缺点有哪些？

系统可用性降低(mq挂掉),数据丢失问题,一致性问题,消息顺序问题,消息积压,消息重复问题等需要考虑，导致系统复杂性增加。

## 3.主流MQ框架的对比

| 特性                    | ActiveMQ                                                     | RabbitMQ                                                     | RocketMQ                                                     | Kafka                                                        |
| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 单机吞吐量              | 万级，吞吐量比RocketMQ和Kafka要低了一个数量级                | 万级，吞吐量比RocketMQ和Kafka要低了一个数量级                | 10万级，RocketMQ也是可以支撑高吞吐的一种MQ                   | 10万级别，这是kafka最大的优点，就是吞吐量高。 一般配合大数据类的系统来进行实时数据计算、日志采集等场景 |
| topic数量对吞吐量的影响 |                                                              |                                                              | topic可以达到几百，几千个的级别，吞吐量会有较小幅度的下降 这是RocketMQ的一大优势，在同等机器下，可以支撑大量的topic | topic从几十个到几百个的时候，吞吐量会大幅度下降 所以在同等机器下，kafka尽量保证topic数量不要过多。如果要支撑大规模topic，需要增加更多的机器资源 |
| 时效性                  | ms级                                                         | 微秒级，这是rabbitmq的一大特点，延迟是最低的                 | ms级                                                         | 延迟在ms级以内                                               |
| 可用性                  | 高，基于主从架构实现高可用性                                 | 高，基于主从架构实现高可用性                                 | 非常高，分布式架构                                           | 非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 |
| 消息可靠性              | 有较低的概率丢失数据                                         |                                                              | 经过参数优化配置，可以做到0丢失                              | 经过参数优化配置，消息可以做到0丢失                          |
| 功能支持                | MQ领域的功能极其完备                                         | 基于erlang开发，所以并发能力很强，性能极其好，延时很低       | MQ功能较为完善，还是分布式的，扩展性好                       | 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准 |
| 优劣势总结              | 非常成熟，功能强大，在业内大量的公司以及项目中都有应用 偶尔会有较低概率丢失消息 而且现在社区以及国内应用都越来越少，官方社区现在对ActiveMQ 5.x维护越来越少，几个月才发布一个版本 而且确实主要是基于解耦和异步来用的，较少在大规模吞吐的场景中使用 | erlang语言开发，性能极其好，延时很低； 吞吐量到万级，MQ功能比较完备 而且开源提供的管理界面非常棒，用起来很好用 社区相对比较活跃，几乎每个月都发布几个版本分 在国内一些互联网公司近几年用rabbitmq也比较多一些 但是问题也是显而易见的，RabbitMQ确实吞吐量会低一些，这是因为他做的实现机制比较重。 而且erlang开发，国内有几个公司有实力做erlang源码级别的研究和定制？如果说你没这个实力的话，确实偶尔会有一些问题，你很难去看懂源码，你公司对这个东西的掌控很弱，基本职能依赖于开源社区的快速维护和修复bug。 而且rabbitmq集群动态扩展会很麻烦，不过这个我觉得还好。其实主要是erlang语言本身带来的问题。很难读源码，很难定制和掌控。 | 接口简单易用，而且毕竟在阿里大规模应用过，有阿里品牌保障 日处理消息上百亿之多，可以做到大规模吞吐，性能也非常好，分布式扩展也很方便，社区维护还可以，可靠性和可用性都是ok的，还可以支撑大规模的topic数量，支持复杂MQ业务场景 而且一个很大的优势在于，阿里出品都是java系的，我们可以自己阅读源码，定制自己公司的MQ，可以掌控 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准JMS规范走的有些系统要迁移需要修改大量代码 还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用RocketMQ挺好的 | kafka的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展 同时kafka最好是支撑较少的topic数量即可，保证其超高吞吐量 而且kafka唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略 这个特性天然适合大数据实时计算以及日志收集 |

不过现在确实越来越多的公司，会去用RocketMQ，确实很不错，但是我提醒一下自己想好社区万一突然黄掉的风险，对自己公司技术实力有绝对自信的，我推荐用RocketMQ，否则回去老老实实用RabbitMQ吧，人是活跃开源社区，绝对不会黄

所以中小型公司，技术实力较为一般，技术挑战不是特别高，用RabbitMQ是不错的选择；大型公司，基础架构研发实力较强，用RocketMQ是很好的选择

如果是大数据领域的实时计算、日志采集等场景，用Kafka是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范



## 4.mq的高可用模式

1. RabbitMQ的高可用性

   rabbitmq有三种模式：单机模式，普通集群模式，镜像集群模式

   1. 单机模式

      一般就是你本地启动了玩玩儿的，没人生产用单机模式

   2. 普通集群模式

      在多台机器上启动多个rabbitmq实例，每个机器启动一个。但是你创建的queue，***只会放在一个rabbtimq实例上***，但是每个实例都同步queue的元数据。消费的时候，*实际上如果连接到了另外一个实例，那么那个实例会从queue所在实例上拉取数据过来*。

      这种方式非真正的集群,没有高可用,实例节点挂掉mq就会不可用,只是提高了连接的吞吐量,而且由于数据位于不同的实例上,当访问不在连接的实例上会有很多这样的mq内部通讯。

   3. 镜像集群模式

      真正的rabbitmq的高可用模式，跟普通集群模式不一样的是，创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，每次写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。

       坏处在于，第一，性能开销也大，消息同步所有机器，导致网络带宽压力和消耗很重！第二，就没有扩展性可言了，如果某个queue负载很重，加机器，新增的机器也包含了这个queue的所有数据，并没有办法线性扩展的queue 

2. kafka的高可用性

   kafka一个最基本的架构认识：多个broker组成，每个broker是一个节点；创建一个topic，这个topic可以划分为多个partition，每个partition可以存在于不同的broker上，每个partition就放一部分数据。

   这是天然的分布式消息队列，就是说一个topic的数据，是分散放在多个机器上的，每个机器就放一部分数据。

   实际上rabbitmq之类的，并不是分布式消息队列，他就是传统的消息队列，只不过提供了一些集群、HA的机制，因为无论怎么玩，rabbitmq一个queue的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个queue的完整数据。

   kafka 0.8以前，是没有HA机制的，就是任何一个broker宕机了，那个broker上的partition就废了，没法写也没法读，没有什么高可用性可言。

   kafka 0.8以后，提供了HA机制，就是replica副本机制。每个partition的数据都会同步到其他机器上，形成自己的多个replica副本。然后所有replica会选举一个leader出来，那么生产和消费都跟这个leader打交道，然后其他replica就是follower。写的时候，leader会负责把数据同步到所有follower上去，读的时候就直接读leader上数据即可。只能读写leader？很简单，要是你可以随意读写每个follower，那么就要care数据一致性的问题，系统复杂度太高，很容易出问题。kafka会均匀的将一个partition的所有replica分布在不同的机器上，这样才可以提高容错性。如果某个broker宕机了，没事儿，那个broker上面的partition在其他机器上都有副本的，如果这上面有某个partition的leader，那么此时会重新选举一个新的leader出来，大家继续读写那个新的leader即可。这就有所谓的高可用性了。

    写数据的时候，生产者就写leader，然后leader将数据落地写本地磁盘，接着其他follower自己主动从leader来pull数据。一旦所有follower同步好数据了，就会发送ack给leader，leader收到所有follower的ack之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）

   消费的时候，只会从leader去读，但是只有一个消息已经被所有follower都同步成功返回ack的时候，这个消息才会被消费者读到。

## 5.消息重复的处理

​	一般mq是不保证发送消息的重复,有极低的概率可能会消息重复.需要靠业务自己保证幂性的,一般使用数据库,redis等来判断幂等。

## 6.消息丢失情况分析

1. rabbitmq

   - 生产者弄丢数据

     因为网络或其他通讯原因,数据半路可能丢失,一般通过

     开启事务（channel.txSelect），然后发送消息，如果消息没有成功被rabbitmq接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。但是问题是，rabbitmq事务机制一搞，基本上吞吐量会下来，因为太耗性能。

     开启confirm模式，每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会给你回传一个ack消息，告诉你说这个消息ok了。如果rabbitmq没能处理这个消息，会回调你一个nack接口，告诉你这个消息接收失败，你可以重试。因为确认模式是异步的,所以一般发送端消息丢失都是使用开启confirm模式进行处理避免消息丢失。


   - rabbitmq弄丢了数据

     一般必须开始rabbitmq持久化,这样就算mq挂了,也可以读取之前存储的数据,一般不会丢失,当然也有极其罕见的情况数据还未写入磁盘mq挂了导致的数据丢失,这种概率极低。

     设置持久化有两个步骤

     第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；

     第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。

     而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。 

   - 消费端弄丢了数据

     一般使用rabbitmq提供的手动ack机制,未被ack的消息,mq会重复进行投递(死信队列).

2. kafka

   - 消费者丢失数据

     如果设置为消费者自动提交offset,此时你刚接到消息还未处理,此消息可能就丢失了,一般的做法是关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。

   - kafka数据丢失

     kafka某个broker宕机，然后重新选举partiton的leader时。此时其他的follower刚好还有些数据没有同步，结果此时leader挂了，然后选举某个follower成leader之后，就少了一些数据。

     此时一般是要求起码设置如下4个参数：

     给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本

     在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧

     在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了

     在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了

     这样配置之后，至少在kafka broker端就可以保证在leader所在broker发生故障，进行leader切换时，数据不会丢失

   - 生产者不会丢数据

     如果按照上述的思路设置了ack=all，一定不会丢，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。

## 7.保证消息的顺序性

​	场景：比如下单操作，下单成功之后，会发布创建订单和扣减库存消息，但扣减库存消息执行会先于创建订单消息，也就说前者执行成功之后，才能执行后者。

​	在 MQ 层面支持消息的顺序处理开销太大，为了极少量的需求，增加整体上的复杂度得不偿失。

所以，**还是在应用层面处理比较好，或者业务逻辑进行处理**。

应用层解决方式：

- **1.** 消息实体中增加：版本号 & 状态机 & msgid & parent_msgid，通过 parent_msgid 判断消息的顺序（需要全局存储，记录消息的执行状态）。当处理完后更新parent_msgid然后触发mq重试机制,继续其他消费
- **2.** “同步执行”：当一个消息执行完之后，再发布下一个消息。
- **3.**确保只有一个消费者消费,消费者内部使用队列,然后启动多个线程消费队列。



## 8.处理消息积压

​	由于某些业务场景导致有些队列没有被消费(消费者挂了,或者消费能力弱),一般处理的方式有如下方式.

消费者积极扩容,当发现百万级的消息积压,首先是先把消费者逻辑处理给修正,想办法临时扩容消费者,加大消费者的处理吞吐量

1）先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉

2）新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量

3）然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue

4）接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据

5）这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据

6）等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息

对于一些消息已被超时丢弃(一般线上不允许设置小时超时),只能通过补录丢失的消息了,在高峰期过后,一点点的将业务数据查询出来,在低谷的时候写个程序模拟生产消息。

这里还是要提醒,要有主动监控消息积压的流程,当发生消息积压的初期就及时处理,如果发现对业务不影响的消息还可以直接删除。



# 分布式搜索引擎



## es的基础知识

- 倒排索引

  也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。

- ES索引

  ES一些概念和数据库进行对比

  Relational DB -> Databases -> Tables -> Rows -> Columns

  Elasticsearch -> Index -> Types -> Documents -> Fields

- es分布式结构

  一个索引可以拆分成多个shard,每个shard存储部分数据。就是说每个shard都有一primary shard，负责写入数据，还有一个或多个replica shard。primary shard写入数据之后,会将数据同步到其他replica shard上去。

- es分布式master节点

  es集群多个节点,会自动选举一个节点为master节点,主要负责管理工作,维护索引元数据拉,切换primary shard和replica shard身份之类。要是master节点宕机了,会重新选举一个节点为master节点。

- es分布式非master

  master节点会让宕机节点上的primary shard的身份转移到其他一个机器上的replica shard。宕机节点重启之后,master节点会控制将缺失的replica shard分配过去,同步后续修改的数据,让集群恢复正常。

## es的工作原理

1. es写数据过程

   1）客户端选择一个node发送请求过去,这个node就被称作coordinating node（协调节点）

   2）coordinating node,对document进行路由,将请求转发给对应的node（有primary shard）

   3）实际的node上的primary shard处理请求,然后将数据同步到replica node

   4）coordinating node,如果发现primary node和所有replica node都处理好之后,就返回响应结果给客户端

2. es读数据过程

   当es写入一个docment时候,会自动给你分配一个全局唯一的doc id,同时es也是根据doc id进行hash路由到对应的primary shard上面去

   1）客户端发送请求到任意一个node，成为coordinate node

   2）coordinate node对document进行路由，将请求转发到对应的node，此时会使用round-robin随机轮询算法，在primary shard以及其所有replica中随机选择一个，让读请求负载均衡

   3）接收请求的node返回document给coordinate node

   4）coordinate node返回document给客户端

3. es搜索数据过程

   1）客户端发送请求到一个coordinate node

   2）协调节点将搜索请求转发到所有的shard对应的primary shard或replica shard也可以

   3）query phase：每个shard将自己的搜索结果（其实就是一些doc id），返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果

   4）fetch phase：接着由协调节点，根据doc id去各个节点上拉取实际的document数据，最终返回给客户端

4. 写数据底层原理

   1）先写入buffer,在buffer里的时候数据是搜索不到的;同时将数据写入translog日志文件

   2）如果buffer快满了,或者到一定时间,就会将buffer数据refresh到一个新的segment file中,但是此时数据不是直接进入segment file的磁盘文件的,而是先进入os cache的,这个过程就是refresh。每隔1秒钟，es将buffer中的数据写入一个新的segment file,每秒钟会产生一个新的磁盘文件,segment file,这个segment file中就存储最近1秒内buffer中写入的数据,但是如果buffer里面此时没有数据,不会执行refresh操作,每秒创建换一个空的segment file,如果buffer里面有数据,默认1秒钟执行一次refresh操作,刷入一个新的segment file中,操作系统里面,磁盘文件其实都有一个东西,叫做os cache,操作系统缓存,就是说数据写入磁盘文件之前,会先进入os cache,先进入操作系统级别的一个内存缓存中去,只要buffer中的数据被refresh操作,刷入os cache中，就代表这个数据就可以被搜索到了,**内存 buffer 生成一个新的 segment，刷到文件系统缓存中，Lucene 即可检索这个新 segment,此时segment位于文件内存缓存中**,为什么叫es是准实时的？NRT，near real-time,准实时。默认是每隔1秒refresh一次的,所以es是准实时的,因为写入的数据1秒之后才能被看到。可以通过es的restful api或者java api，手动执行一次refresh操作，就是手动将buffer中的数据刷入os cache中，让数据立马就可以被搜索到。只要数据被输入os cache中，buffer就会被清空了，因为不需要保留buffer了，数据在translog里面已经持久化到磁盘去一份了

   3）只要数据进入os cache,此时就可以让这个segment file的数据对外提供搜索了

   4）重复1~3步骤，新的数据不断进入buffer和translog,不断将buffer数据写入一个又一个新的segment file中去，每次refresh完buffer清空，translog保留。随着这个过程推进，translog会变得越来越大。当translog达到一定长度的时候，就会触发commit操作。buffer中的数据，每隔1秒就被刷到os cache中去，然后这个buffer就被清空了。所以说这个buffer的数据始终是可以保持住不会填满es进程的内存的。每次一条数据写入buffer,同时会写入一条日志到translog日志文件中去,所以这个translog日志文件是不断变大的,当translog日志文件大到一定程度的时候，就会执行commit操作。

   5）commit操作发生第一步，就是将buffer中现有数据refresh到os cache中去，清空buffer

   6）将一个commit point写入磁盘文件，里面标识着这个commit point对应的**所有os cache内存segment file**

   7）强行将os cache中目前所有的数据都fsync到磁盘文件中去,translog日志文件的作用是什么？就是在你执行commit操作之前，数据要么是停留在buffer中，要么是停留在os cache中，无论是buffer还是os cache都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件，translog日志文件中，一旦此时机器宕机，再次重启的时候，es会自动读取translog日志文件中的数据，恢复到内存buffer和os cache中去。

   commit操作：

   1、写commit point；

   2、将os cache数据fsync强刷到磁盘上去；

   3、清空translog日志文件

   8）将现有的translog清空，然后再次重启启用一个translog，此时commit操作完成。默认每隔30分钟会自动执行一次commit，但是如果translog过大，也会触发commit。整个commit的过程，叫做flush操作。我们可以手动执行flush操作，就是将所有os cache数据刷到磁盘文件中去。不叫做commit操作，flush操作。es中的flush操作，就对应着commit的全过程。我们也可以通过es api，手动执行flush操作，手动将os cache中的数据fsync强刷到磁盘上去，记录一个commit point，清空translog日志文件。

   9）translog其实也是先写入os cache的，默认每隔5秒刷一次到磁盘中去，所以默认情况下，可能有5秒的数据会仅仅停留在buffer或者translog文件的os cache中，如果此时机器挂了，会丢失5秒钟的数据。但是这样性能比较好，最多丢5秒的数据。也可以将translog设置成每次写操作必须是直接fsync到磁盘，但是性能会差很多。其实es第一是准实时的，数据写入1秒后可以搜索到；可能会丢失数据的，你的数据有5秒的数据，停留在buffer、translog os cache、segment file os cache中，有5秒的数据不在磁盘上，此时如果宕机，会导致5秒的数据丢失。如果你希望一定不能丢失数据的话，你可以设置个参数每次写入一条数据，都是写入buffer，同时写入磁盘上的translog，但是这会导致写性能、写入吞吐量会下降一个数量级。

   Elasticsearch 2.0 新加入的特性。为了保证不丢数据，每次 index、bulk、delete、update 完成的时候，一定触发刷新 translog 到磁盘上，才给请求返回 200 OK。这个改变在提高数据安全性的同时当然也降低了一点性能。

   10）如果是删除操作，commit的时候会生成一个.del文件，里面将某个doc标识为deleted状态，那么搜索的时候根据.del文件就知道这个doc被删除了

   11）如果是更新操作，就是将原来的doc标识为deleted状态，然后新写入一条数据

   12）buffer每次refresh一次，就会产生一个segment file，所以默认情况下是1秒钟一个segment file，segment file会越来越多，此时会定期执行merge

   13）每次merge的时候，会将多个segment file合并成一个，同时这里会将标识为deleted的doc给物理删除掉，然后将新的segment file写入磁盘，这里会写一个commit point，标识所有新的segment file，然后打开segment file供搜索使用，同时删除旧的segment file。

   es里的写流程，有4个底层的核心概念，refresh、flush、translog、merge,当segment file多到一定程度的时候，es就会自动触发merge操作，将多个segment file给merge成一个segment file。



## es搜索的性能优化(几十亿)

es搜索优化不是银弹,关键的底层性能影响就是内存的es缓存(filesystem cache)

1. 增加filesystem cache的值设置足够大,确保es机器内存足够大,如果es未命中filesystem cache数据,会到磁盘中读取十分影响读取的性能。(filesystem cache尽量足够大)

2. 尽量少的字段存放到es这样filesystem cache能更大存放es数据,真正大的数据存放到外部mysql或hbase中通过es中的关键字段到外置存储中查询。(filesystem cache尽量保留关键)

3. 数据预热,如果实在是有数据被存放在了磁盘中,对于经常访问的数据,可以定时查询一下,保证filesystem cache中的都基本是热数据,提升体验。(filesystem cach尽量多存放热数据)

4. 冷热分离,将热数据和冷数据尽量水平拆分,这样可以保证尽可能多的热数据位于filesystem cache中。(filesystem cach尽量多存放热数据)

5. 优化document模型设计,对于原始数据有关联的结果,尽量在写入的时候就将结果写入,尽量避免使用复杂关联的查询。复杂的计算尽量在程序中做。(es尽量少的计算合并读取)

6. 分页的优化,es的分页,分页深度越大越慢,因为是分布式存储数据,分页深度大的情况下,协调节点依然会将pagesize*pageindex的量的数据查询出来然后做合并排序等操作,再取出分页的数据。

   对于不需要跳转,只需要一页一页下翻的分页,可以使用scroll api,其原理是使用了es数据快照,将查询出来的数据进行快照,然后通过游标一页一页的展示,缺点是无法指定页数以及页跳转。



#  分布式缓存









# 分布式系统

